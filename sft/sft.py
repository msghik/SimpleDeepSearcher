# minor update
import os
import copy
import json
import logging
from tqdm import tqdm
from dataclasses import dataclass, field
from typing import Optional, Dict, Sequence

import torch
from torch.utils.data import random_split
from torch.nn.utils.rnn import pad_sequence
import transformers
from torch.utils.data import Dataset
from transformers import Trainer
import random
from typing import List, Optional, Tuple, Union
from transformers import AutoModelForCausalLM, TrainingArguments
from datasets import load_dataset
from transformers import DataCollatorForSeq2Seq
import shutil

# from liger_kernel.transformers import AutoLigerKernelForCausalLM


import matplotlib.pyplot as plt
import numpy as np


@dataclass
class ModelArguments:
    model_name_or_path: Optional[str] = field(default="facebook/opt-125m")
    # flash_attention: Optional[bool] = field(default=False)
    tokenizer_name_or_path: Optional[str] = field(default=None)


@dataclass
class DataArguments:
    data_path: str = field(
        default=None, metadata={"help": "Path to the training data."}
    )
    prompt_type: Optional[str] = field(default="instruction")
    dailog_augmentation: Optional[bool] = field(default=False)
    other_type_data: Optional[str] = field(
        default=None,
        metadata={
            "help": "The path of other type of data"
        },
    )


@dataclass
class TrainingArguments(transformers.TrainingArguments):
    cache_dir: Optional[str] = field(default=None)
    optim: str = field(default="adamw_torch")
    model_max_length: int = field(
        default=512,
        metadata={
            "help": "Maximum sequence length. Sequences will be right padded (and possibly truncated)."
        },
    )


IGNORE_INDEX = -100
MAX_LENGTH = 2000

def process(sample, tokenizer):
    # build inputs with format `<bos> X Y <eos>` and labels with format `<ignore> ... <ignore> Y <eos>`
    # for multiturn examples, we only mask the prompt part in each prompt-response pair.
    source = sample["input"]

    source = tokenizer.apply_chat_template(
        [
            {'role': 'user', 'content': source}
        ],
        tokenize=False, add_generation_prompt=True
    )

    source = tokenizer(source, add_special_tokens=False)["input_ids"]
    target = [IGNORE_INDEX] * len(source)
    for output in sample["output"]:
        for k, v in output.items():
            if v is None:
                continue
            v_tokens = tokenizer(v, add_special_tokens=False)["input_ids"]
            if k in ["gen"]:
                source += v_tokens
                target += v_tokens
            elif k in ["doc_gen"]:
                source += v_tokens
                target += [IGNORE_INDEX] * len(v_tokens)
    input_ids = source
    labels = target

    input_ids.append(tokenizer.eos_token_id)
    labels.append(tokenizer.eos_token_id)

    result = {
        "input_ids": input_ids,
        "attention_mask": [1] * len(input_ids),
        "labels": labels,
    }
    # print(result)
    return result


def process_other_data(sample, tokenizer): # without mask

    source = sample["prompt"]
    source = tokenizer.apply_chat_template(
        [
            {'role': 'user', 'content': source}
        ],
        tokenize=False, add_generation_prompt=True
    )

    source = tokenizer(source, add_special_tokens=False)["input_ids"]
    target = [IGNORE_INDEX] * len(source)

    output = sample["output"]
    output = tokenizer(output, add_special_tokens=False)["input_ids"]

    source += output
    target += output
    
    input_ids = source
    labels = target

    input_ids.append(tokenizer.eos_token_id)
    labels.append(tokenizer.eos_token_id)

    result = {
        "input_ids": input_ids,
        "attention_mask": [1] * len(input_ids),
        "labels": labels,
    }
    # print(result)
    return result



def print_function(example, tokenizer):
    print("input_ids:\n{}".format(example["input_ids"]))
    print("inputs:\n{}".format(tokenizer.decode(example["input_ids"], skip_special_tokens=False)))
    print("label_ids:\n{}".format(example["labels"]))
    print("labels:\n{}".format(
        tokenizer.decode(list(filter(lambda x: x != IGNORE_INDEX, example["labels"])), skip_special_tokens=False)
    ))


def get_dataset(file_path, tokenizer, other_dataset=False):
    dataset = load_dataset('json', data_files=file_path)
    train_dataset = dataset["train"]
    file_name = os.path.basename(file_path)
    dataset_name = os.path.splitext(file_name)[0]

    if other_dataset:
        tokenized_dataset = train_dataset.map(process_other_data, fn_kwargs={'tokenizer': tokenizer}, num_proc=1, load_from_cache_file=False)
    else:
        tokenized_dataset = train_dataset.map(process, fn_kwargs={'tokenizer': tokenizer}, num_proc=1, load_from_cache_file=False)
    print_function(next(iter(tokenized_dataset)), tokenizer)
    print(f"len of dataset before filter: {len(tokenized_dataset)}")
    
    filtered_dataset = []
    for item in tokenized_dataset:
        if len(item["input_ids"]) <= 10000:
            filtered_dataset.append(item)
    print(f"len of dataset after filter: {len(filtered_dataset)}")
    return filtered_dataset


def train():
    parser = transformers.HfArgumentParser(
        (ModelArguments, DataArguments, TrainingArguments)
    )
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    print("==========Model Args=========")
    print(model_args)
    print("==========Data Args=========")
    print(data_args)
    print("==========Training Args=========")
    print(training_args)

    use_cache = True
    if training_args.gradient_checkpointing:
        use_cache = False # use_cache与gradient_checkpointing不能同时设置为true
    model = AutoModelForCausalLM.from_pretrained(
        model_args.model_name_or_path,
        _attn_implementation="flash_attention_2",
        use_cache=use_cache, 
        #  save_only_model=True
    ).float()

    if model_args.tokenizer_name_or_path is None:
        model_args.tokenizer_name_or_path = model_args.model_name_or_path
    tokenizer = transformers.AutoTokenizer.from_pretrained(
        model_args.tokenizer_name_or_path, model_max_length=training_args.model_max_length
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    dataset = get_dataset(data_args.data_path, tokenizer)
    if data_args.other_type_data:
        dataset_other = get_dataset(data_args.other_type_data, tokenizer, True)
        dataset = dataset + dataset_other
    print(f"dataset length: {len(dataset)}")

    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        label_pad_token_id=-100,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        tokenizer=tokenizer,
        data_collator=data_collator,
        train_dataset=dataset,
    )
    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
    trainer.save_model(training_args.output_dir)
    trainer.save_state()


if __name__ == "__main__":
    torch.manual_seed(42)
    train()